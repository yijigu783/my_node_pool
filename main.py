import requests
import base64
import re
import json
import urllib.parse

# è®¢é˜…æºåˆ—è¡¨
URLS = [
    "https://raw.githubusercontent.com/ebrasha/free-v2ray-public-list/refs/heads/main/all_extracted_configs.txt",
    "https://raw.githubusercontent.com/anaer/Sub/main/clash.yaml",
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
    "https://raw.githubusercontent.com/free-nodes/v2rayfree/main/v2",
    "https://raw.githubusercontent.com/free-nodes/clashfree/main/clash.yml",
    "https://bulinkbulink.com/freefq/free/master/v2",
    "https://raw.githubusercontent.com/chengaopan/AutoMergePublicNodes/master/list.txt",
    "https://raw.githubusercontent.com/crossxx-labs/free-proxy/main/clash/vmess.yml"
]

# å…³é”®è¯æ˜ å°„
KEYWORD_MAP = {
    'HK': ['hk', 'hongkong', 'hong kong', 'é¦™', 'æ¸¯'],
    'JP': ['jp', 'japan', 'tokyo', 'osaka', 'æ—¥'],
    'US': ['us', 'united states', 'america', 'ç¾', 'los angeles'],
    'SG': ['sg', 'singapore', 'æ–°', 'ç‹®åŸ'],
    'TW': ['tw', 'taiwan', 'taipei', 'å°'],
    'KR': ['kr', 'korea', 'seoul', 'éŸ©'],
    'DE': ['de', 'germany', 'frankfurt', 'å¾·'],
    'GB': ['uk', 'gb', 'united kingdom', 'london', 'è‹±'],
    'RU': ['ru', 'russia', 'moscow', 'ä¿„'],
    'FR': ['fr', 'france', 'paris', 'æ³•'],
    'CA': ['ca', 'canada', 'åŠ '],
}

FLAG_MAP = {
    'HK': 'ğŸ‡­ğŸ‡°', 'JP': 'ğŸ‡¯ğŸ‡µ', 'US': 'ğŸ‡ºğŸ‡¸', 'SG': 'ğŸ‡¸ğŸ‡¬', 'TW': 'ğŸ‡¹ğŸ‡¼',
    'KR': 'ğŸ‡°ğŸ‡·', 'DE': 'ğŸ‡©ğŸ‡ª', 'GB': 'ğŸ‡¬ğŸ‡§', 'RU': 'ğŸ‡·ğŸ‡º', 'FR': 'ğŸ‡«ğŸ‡·', 'CA': 'ğŸ‡¨',
    'UNKNOWN': 'ğŸ³ï¸'
}

def get_content(url):
    try:
        print(f"[-] ä¸‹è½½ä¸­: {url}...", flush=True)
        resp = requests.get(url, timeout=10)
        if resp.status_code == 200:
            return resp.text
        return ""
    except Exception as e:
        print(f"    [!] ä¸‹è½½å¼‚å¸¸: {e}", flush=True)
        return ""

def identify_country(text):
    text = text.lower()
    for code, keywords in KEYWORD_MAP.items():
        for kw in keywords:
            if kw in text:
                return code
    return 'UNKNOWN'

def rename_vmess(link):
    try:
        if not link.startswith("vmess://"): return link
        b64_str = link[8:]
        missing_padding = len(b64_str) % 4
        if missing_padding: b64_str += '=' * (4 - missing_padding)
        json_str = base64.b64decode(b64_str).decode('utf-8', errors='ignore')
        config = json.loads(json_str)
        original_ps = config.get('ps', '')
        address = config.get('add', '')
        country = identify_country(original_ps)
        if country == 'UNKNOWN': country = identify_country(address)
        flag = FLAG_MAP.get(country, 'ğŸ³ï¸')
        clean_ps = original_ps[:25]
        # ã€ä¿®å¤ã€‘å¦‚æœåå­—æ˜¯ç©ºçš„ï¼Œç»™ä¸€ä¸ªé»˜è®¤åå­—
        if not clean_ps: clean_ps = "Node"
        new_ps = f"{flag} {country} {clean_ps}"
        config['ps'] = new_ps
        new_json = json.dumps(config, ensure_ascii=False)
        new_b64 = base64.b64encode(new_json.encode('utf-8')).decode('utf-8')
        return f"vmess://{new_b64}"
    except:
        return link

def rename_url_struct(link):
    try:
        parsed = urllib.parse.urlparse(link)
        original_name = urllib.parse.unquote(parsed.fragment)
        host = parsed.hostname or ""
        country = identify_country(original_name)
        if country == 'UNKNOWN': country = identify_country(host)
        flag = FLAG_MAP.get(country, 'ğŸ³ï¸')
        # ã€ä¿®å¤ã€‘å¦‚æœåå­—å¤ªé•¿è¿›è¡Œæˆªæ–­ï¼Œå¦‚æœä¸ºç©ºç»™é»˜è®¤å€¼
        name_clean = original_name[:20] if original_name else "Node"
        new_name = f"{flag} {country} {name_clean}"
        new_parsed = parsed._replace(fragment=urllib.parse.quote(new_name))
        return urllib.parse.urlunparse(new_parsed)
    except:
        return link

def process_nodes(content):
    processed_nodes = set()
    
    # ã€æ ¸å¿ƒä¿®å¤ã€‘è¿™é‡Œä½¿ç”¨äº† (?:...) éæ•è·ç»„ï¼Œç¡®ä¿ findall è¿”å›å®Œæ•´çš„é“¾æ¥å­—ç¬¦ä¸²
    # åŒæ—¶å¢å¼ºäº†æ­£åˆ™ï¼Œé˜²æ­¢åŒ¹é…åˆ°ç©ºé“¾æ¥
    raw_links = re.findall(r'(?:vmess|vless|ss|trojan|hysteria2?)://[a-zA-Z0-9\-\._~%!$&\'()*+,;=:@/?#]+', content)
    
    for link in raw_links:
        # è¿‡æ»¤æ‰æ˜¾ç„¶å¤ªçŸ­çš„æ— æ•ˆé“¾æ¥
        if len(link) < 15: continue
        
        if link.startswith("vmess://"):
            new_link = rename_vmess(link)
        else:
            new_link = rename_url_struct(link)
        processed_nodes.add(new_link)

    # å¤„ç† Base64 è®¢é˜…å†…å®¹
    try:
        clean_content = content.replace(' ', '').replace('\n', '')
        if len(clean_content) % 4 != 0:
            clean_content += '=' * (4 - len(clean_content) % 4)
        decoded = base64.b64decode(clean_content).decode('utf-8', errors='ignore')
        
        # é€’å½’æå–
        decoded_links = re.findall(r'(?:vmess|vless|ss|trojan|hysteria2?)://[a-zA-Z0-9\-\._~%!$&\'()*+,;=:@/?#]+', decoded)
        for link in decoded_links:
            if len(link) < 15: continue
            if link.startswith("vmess://"):
                new_link = rename_vmess(link)
            else:
                new_link = rename_url_struct(link)
            processed_nodes.add(new_link)
    except:
        pass

    return processed_nodes

def main():
    print("=== ä¿®å¤ç‰ˆè„šæœ¬å¼€å§‹è¿è¡Œ ===", flush=True)
    all_nodes = set()

    for url in URLS:
        content = get_content(url)
        if not content: continue
        
        nodes = process_nodes(content)
        if nodes:
            print(f"    > æˆåŠŸå¤„ç† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            all_nodes.update(nodes)

    print(f"=== å®Œæˆ ===")
    print(f"å…±è·å– {len(all_nodes)} ä¸ªèŠ‚ç‚¹")

    final_text = "\n".join(all_nodes)
    
    # ä¿å­˜æ–‡ä»¶
    with open("nodes_plain.txt", "w", encoding="utf-8") as f:
        f.write(final_text)

    final_base64 = base64.b64encode(final_text.encode('utf-8')).decode('utf-8')
    with open("subscribe.txt", "w", encoding="utf-8") as f:
        f.write(final_base64)

if __name__ == "__main__":
    main()
